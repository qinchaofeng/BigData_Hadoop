hive.metastore.uris：指定hive元数据访问路径，如果为空则是本地模式，否则为远程模式
hive.metastore.warehouse.dir：（HDFS上的）数据目录，可以在3个地方保存 默认值是/user/hive/warehouse
hive.exec.scratchdir：（HDFS上的）临时文件目录                        默认值是/tmp/hive-${user.name}


1.安装mysql
wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-community-server-5.7.17-1.el6.x86_64.rpm --no-check-certificate
yum install -y mysql-server mysql mysql-deve
[root@node01 ~]# groupadd mysql
[root@node01 ~]# useradd -g mysql mysql
[root@node01 lib]# chown -R mysql:mysql /var/lib/mysql
[root@node01 ~]# cat /etc/my.cnf 
[mysqld]
datadir=/var/lib/mysql
#socket=/var/lib/mysql/mysql.sock
user=mysql
# Disabling symbolic-links is recommended to prevent assorted security risks
symbolic-links=0
[root@node01 ~]# service mysqld start
Starting mysqld:                                           [  OK  ]
#设置密码
mysqladmin -u root password 'root'
#赋给权限
create user 'hive' identified by 'hive'
#grant all privileges on *.* to 'hive' with grant option;
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;
flush privileges;
#测试
[root@node02 opt]# mysql -h node01 -u hive -p 



2.环境变量                 
echo 'export HIVE_HOME=/app/apache-hive-2.1.1-bin' >> /etc/profile
echo 'export PATH=$PATH:$HIVE_HOME/bin' >> /etc/profile
source /etc/profile

3.修改hive-env.sh
cp /app/apache-hive-2.1.1-bin/conf/hive-env.sh.template /app/apache-hive-2.1.1-bin/conf/hive-env.sh

4.mysql驱动:
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.41.tar.gz
cp mysql-connector-java-5.1.41-bin.jar /app/apache-hive-2.1.1-bin/lib/

5.修改hive-site.xml
#cp hive-default.xml.template hive-site.xml
<configuration>
<property>  
  <name>javax.jdo.option.ConnectionURL</name>  
  <value>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true</value>  
  <description>JDBC connect string for a JDBC metastore</description>  
</property>  
  
<property>  
  <name>javax.jdo.option.ConnectionDriverName</name>  
  <value>com.mysql.jdbc.Driver</value>  
  <description>Driver class name for a JDBC metastore</description>  
</property>  
  
<property>  
  <name>javax.jdo.option.ConnectionUserName</name>  
  <value>hive</value>  
  <description>username to use against metastore database</description>  
</property>  
  
<property>  
  <name>javax.jdo.option.ConnectionPassword</name>  
  <value>hive</value>  
  <description>password to use against metastore database</description>  
</property>  
</configuration>

6.复制hive到其他节点
scp -r /app/apache-hive-2.1.1-bin/  node02:/app/
scp -r /app/apache-hive-2.1.1-bin/  node03:/app/
scp -r /app/apache-hive-2.1.1-bin/  node04:/app/

7.创建目录:
hadoop fs -mkdir /hive
hadoop fs -mkdir /hive/warehouse
hadoop fs -chmod g+w /hive
hadoop fs -chmod g+w /hive/warehouse

8.初始化元数据
schematool -initSchema -dbType mysql

(1)
file:/app/apache-hive-2.1.1-bin/conf/hive-site.xml; 
lineNumber: 7; columnNumber: 2; The markup in the document following the root element must be well-formed.

scp hive-site.xml  node02:/app/apache-hive-2.1.1-bin/conf/hive-site.xml
scp hive-site.xml  node03:/app/apache-hive-2.1.1-bin/conf/hive-site.xml
scp hive-site.xml  node04:/app/apache-hive-2.1.1-bin/conf/hive-site.xml

(2)
[root@node01 conf]# schematool -initSchema -dbType mysql
Metastore connection URL:	 jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true
Metastore Connection Driver :	 com.mysql.jdbc.Driver
Metastore connection User:	 hive
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.
Underlying cause: java.sql.SQLException : Access denied for user 'hive'@'localhost' (using password: YES)
SQL Error code: 1045
Use --verbose for detailed stacktrace.
*** schemaTool failed ***

(3)flush privileges;
<property>  
  <name>javax.jdo.option.ConnectionURL</name>  
  <value>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true</value>  
  <description>JDBC connect string for a JDBC metastore</description>  
</property>  

GRANT ALL PRIVILEGES ON *.* TO 'hive'@'node01' IDENTIFIED BY 'hive' WITH GRANT OPTION;
flush privileges;
(4)

[root@node01 conf]# schematool -initSchema -dbType mysql
Initialization script hive-schema-2.1.0.mysql.sql
Initialization script completed
schemaTool completed

9.登陆
Logging initialized using configuration in jar:file:/app/apache-hive-2.1.1-bin/lib/hive-common-2.1.1.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> 


10.hive-site.xml的修改:

hive.metastore.uris：指定hive元数据访问路径，如果为空则是本地模式，否则为远程模式
hive.metastore.warehouse.dir：（HDFS上的）数据目录，可以在3个地方保存 默认值是/user/hive/warehouse
hive.exec.scratchdir：（HDFS上的）临时文件目录                        默认值是/tmp/hive-${user.name}

hadoop fs -mkdir /hive
hadoop fs -mkdir /hive/warehouse
hadoop fs -chmod g+w /hive
hadoop fs -chmod g+w /tmp
hadoop fs -chmod g+r /tmp
hadoop fs -chmod g+w /hive/warehouse



<property>  
  <name>hive.metastore.warehouse.dir</name>  
  <value>/hive/warehouse</value>  
</property>

scp hive-site.xml  node02:/app/apache-hive-2.1.1-bin/conf/hive-site.xml
scp hive-site.xml  node03:/app/apache-hive-2.1.1-bin/conf/hive-site.xml
scp hive-site.xml  node04:/app/apache-hive-2.1.1-bin/conf/hive-site.xml

11.建表:
create table page_view  
(  
page_id bigint comment '页面ID',  
page_name string comment '页面名称',  
page_url string comment '页面URL'  
)  
comment '页面视图'  
partitioned by (ds string comment '当前时间，用于分区字段')  
row format delimited  
stored as rcfile  
location '/hive/warehouse'; 


create table test_ds  
(  
  id int comment '用户ID',  
  name string comment '用户名称'  
)  
comment '测试分区表'  
partitioned by(ds string comment '时间分区字段')  
clustered by(id) sorted by(name) into 32 buckets      
row format delimited   
fields terminated by '\t'  
stored as rcfile;




11.hive hwi
wget http://mirror.bit.edu.cn/apache/hive/stable/apache-hive-1.2.0-src.tar.gz
[root@node01 hwi]# pwd
/app/hive_src/apache-hive-2.1.1-src
jar -cvf hive-hwi-2.1.1.war hwi/*
cp hive-hwi-2.1.1.war /app/apache-hive-2.1.1-bin/lib/

hive-site.xml:

<property>
    <name>hive.hwi.war.file</name>
    <value>lib/hive-hwi-2.1.1.war</value>
    <description>This sets the path to the HWI war file, relative to ${HIVE_HOME}. </description>
</property>
<property>
    <name>hive.hwi.listen.host</name>
    <value>192.168.85.129</value>
    <description>This is the host address the Hive Web Interface will listen on</description>
</property>
<property>
    <name>hive.hwi.listen.port</name>
    <value>9999</value>
    <description>This is the port the Hive Web Interface will listen on</description>
</property>

(1)启动:
[root@node01 lib]# pwd
/app/apache-hive-2.1.1-bin/lib
[root@node01 lib]# nohup hive --service hwi &
http://192.168.85.129:9999/hwi

(2)
Problem accessing /hwi/web/. Reason:

    Unable to find a javac compiler;
com.sun.tools.javac.Main is not on the classpath.
Perhaps JAVA_HOME does not point to the JDK

(3)
jar -cvf hive-hwi-2.1.1.war hwi/*
cp hive-hwi-2.1.1.war /app/apache-hive-2.1.1-bin/lib/
cd /app/apache-hive-2.1.1-bin/lib
nohup hive --service hwi &

(4)
jar cvfM0 hive-hwi-2.1.1.war -C web/ .
cp hive-hwi-2.1.1.war /app/apache-hive-2.1.1-bin/lib/
hive --service hwi

http://192.168.85.129:9999/hwi/

    Unable to find a javac compiler;
com.sun.tools.javac.Main is not on the classpath.
Perhaps JAVA_HOME does not point to the JDK

(5)
hive --service metastore
hive --service hwi 

echo 'export JAVA_HOME=/app/jdk1.8.0_121'  >>/app/apache-hive-2.1.1-bin/conf/hive-env.sh

(6)
This is common and has a number of causes; the usual 
solutions are to read the manual pages then download and
install needed JAR files, or fix the build file: 
 - You have misspelt 'componentdef'.
   Fix: check your spelling.
 - The task needs an external JAR file to execute
     and this is not found at the right place in the classpath.
   Fix: check the documentation for dependencies.
   Fix: declare the task.
 - The task is an Ant optional task and the JAR file and/or libraries
     implementing the functionality were not found at the time you
     yourself built your installation of Ant from the Ant sources.
   Fix: Look in the ANT_HOME/lib for the 'ant-' JAR corresponding to the
     task and make sure it contains more than merely a META-INF/MANIFEST.MF.
     If all it contains is the manifest, then rebuild Ant with the needed
     libraries present in ${ant.home}/lib/optional/ , or alternatively,
     download a pre-built release version from apache.org
 - The build file was written for a later version of Ant
   Fix: upgrade to at least the latest release version of Ant
 - The task is not an Ant core or optional task 
     and needs to be declared using <taskdef>.
 - You are attempting to use a task defined using 
    <presetdef> or <macrodef> but have spelt wrong or not 
   defined it at the point of use

Remember that for JAR files to be visible to Ant tasks implemented
in ANT_HOME/lib, the files must be in the same directory or on the
classpath


(7)
ln -s $JAVA_HOME/lib/tools.jar $HIVE_HOME/lib/

(8)
http://192.168.85.129:9999/hwi/index.jsp

12.hwi的使用:
(1)http://192.168.85.129:9999/hwi/diagnostics.jsp

##System.getProperties()
java.util.logging.config.file	/app/apache-hive-2.1.1-bin/conf/parquet-logging.properties
user.dir	/app/hive_src/apache-hive-2.1.1-src/hwi/src/java/org/apache/hadoop/hive/hwi
sun.java.command	org.apache.hadoop.util.RunJar /app/apache-hive-2.1.1-bin/lib/hive-hwi-2.1.1.jar org.apache.hadoop.hive.hwi.HWIServer

##System.getenv()
HADOOP_CONF_DIR	/app/hadoop-2.7.3/etc/hadoop
TZ	Asia/Shanghai
LD_LIBRARY_PATH	:/app/hadoop-2.7.3/lib/native
HWI_JAR_FILE	/app/apache-hive-2.1.1-bin/lib/hive-hwi-2.1.1.jar
PWD	/app/hive_src/apache-hive-2.1.1-src/hwi/src/java/org/apache/hadoop/hive/hwi
HADOOP_YARN_HOME	/app/hadoop-2.7.3
HIVE_HOME	/app/apache-hive-2.1.1-bin
LANG	en_US.UTF-8

(2)
http://192.168.85.129:9999/hwi/show_database.jsp?db=default
